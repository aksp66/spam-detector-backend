# app.py - Backend FastAPI pour le d√©tecteur de spam

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import pickle
import os
import requests
import io

# ============================================
# CONFIGURATION DE L'APPLICATION
# ============================================

app = FastAPI(title="D√©tecteur de Spam SMS", version="1.0.0")

# Configuration CORS pour permettre au frontend React de communiquer
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifiez l'URL exacte de votre frontend
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# MOD√àLES DE DONN√âES (pour l'API)
# ============================================

class Message(BaseModel):
    """Structure des donn√©es envoy√©es par le frontend"""
    text: str

class PredictionResponse(BaseModel):
    """Structure de la r√©ponse de pr√©diction"""
    text: str
    prediction: str  # "spam" ou "ham"
    confidence: float  # Score de confiance (0-100%)

# ============================================
# VARIABLES GLOBALES (mod√®le et vectoriseur)
# ============================================

model = None
vectorizer = None
model_stats = {}

# ============================================
# √âTAPE 1 : CHARGEMENT DES DONN√âES
# ============================================

def load_dataset():
    """
    Charge le dataset SMS Spam Collection depuis UCI
    Format : label\tmessage
    """
    print("üì• T√©l√©chargement du dataset...")
    
    # URL du dataset SMS Spam Collection
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"
    
    try:
        # T√©l√©chargement
        response = requests.get(url)
        response.raise_for_status()
        
        # Extraction du fichier ZIP
        import zipfile
        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:
            # Le fichier s'appelle 'SMSSpamCollection'
            with zip_file.open('SMSSpamCollection') as f:
                # Lecture avec pandas (s√©parateur = tabulation)
                df = pd.read_csv(f, sep='\t', names=['label', 'message'], encoding='latin-1')
        
        print(f"‚úÖ Dataset charg√© : {len(df)} messages")
        print(f"   - Ham (non-spam) : {len(df[df['label']=='ham'])}")
        print(f"   - Spam : {len(df[df['label']=='spam'])}")
        
        return df
    
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement : {e}")
        # Dataset de secours pour les tests
        return pd.DataFrame({
            'label': ['ham', 'spam', 'ham', 'spam'],
            'message': [
                'Hello, how are you?',
                'FREE! Click here to win $1000 now!!!',
                'See you tomorrow at the meeting',
                'Congratulations! You have won a free iPhone. Call now!'
            ]
        })

# ============================================
# √âTAPE 2 : PR√âTRAITEMENT
# ============================================

def preprocess_data(df):
    """
    Nettoie et pr√©pare les donn√©es
    - Convertit les labels en 0/1
    - V√©rifie l'absence de valeurs manquantes
    """
    print("üßπ Pr√©traitement des donn√©es...")
    
    # Conversion des labels : ham=0, spam=1
    df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})
    
    # Suppression des √©ventuelles lignes vides
    df = df.dropna()
    
    print(f"‚úÖ Pr√©traitement termin√© : {len(df)} messages pr√™ts")
    return df

# ============================================
# √âTAPE 3 : VECTORISATION (TF-IDF)
# ============================================

def create_features(X_train, X_test):
    """
    Convertit les textes en vecteurs num√©riques avec TF-IDF
    
    TF-IDF = Term Frequency - Inverse Document Frequency
    - Donne plus de poids aux mots rares et discriminants
    - R√©duit l'importance des mots tr√®s fr√©quents
    """
    print("üî¢ Vectorisation TF-IDF...")
    
    # Cr√©ation du vectoriseur
    global vectorizer
    vectorizer = TfidfVectorizer(
        lowercase=True,           # Convertit en minuscules
        stop_words='english',     # Supprime les mots courants (the, is, at...)
        max_features=3000,        # Garde les 3000 mots les plus importants
        ngram_range=(1, 2)        # Utilise les mots seuls et les paires de mots
    )
    
    # Apprentissage et transformation sur les donn√©es d'entra√Ænement
    X_train_vec = vectorizer.fit_transform(X_train)
    
    # Transformation (sans r√©-apprentissage) sur les donn√©es de test
    X_test_vec = vectorizer.transform(X_test)
    
    print(f"‚úÖ Vectorisation termin√©e : {X_train_vec.shape[1]} features")
    return X_train_vec, X_test_vec

# ============================================
# √âTAPE 4 : ENTRA√éNEMENT DU MOD√àLE
# ============================================

def train_model(X_train, y_train, X_test, y_test):
    """
    Entra√Æne un mod√®le Multinomial Na√Øve Bayes
    
    Pourquoi Na√Øve Bayes ?
    - Tr√®s efficace sur les donn√©es textuelles
    - Rapide √† entra√Æner
    - Performant m√™me avec peu de donn√©es
    """
    print("ü§ñ Entra√Ænement du mod√®le Na√Øve Bayes...")
    
    global model, model_stats
    
    # Cr√©ation du mod√®le
    model = MultinomialNB(alpha=0.1)  # alpha = param√®tre de lissage
    
    # Entra√Ænement
    model.fit(X_train, y_train)
    
    # Pr√©dictions sur le jeu de test
    y_pred = model.predict(X_test)
    
    # Calcul des m√©triques de performance
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred, target_names=['Ham', 'Spam'])
    
    # Sauvegarde des statistiques
    model_stats = {
        'accuracy': float(accuracy),
        'confusion_matrix': conf_matrix.tolist(),
        'classification_report': class_report
    }
    
    print(f"‚úÖ Mod√®le entra√Æn√© avec succ√®s !")
    print(f"   Pr√©cision globale : {accuracy*100:.2f}%")
    print("\nüìä Rapport de classification :")
    print(class_report)
    
    return model

# ============================================
# √âTAPE 5 : SAUVEGARDE DU MOD√àLE
# ============================================

def save_model():
    """Sauvegarde le mod√®le et le vectoriseur pour r√©utilisation"""
    print("üíæ Sauvegarde du mod√®le...")
    
    with open('spam_model.pkl', 'wb') as f:
        pickle.dump(model, f)
    
    with open('vectorizer.pkl', 'wb') as f:
        pickle.dump(vectorizer, f)
    
    print("‚úÖ Mod√®le sauvegard√©")

def load_model():
    """Charge le mod√®le sauvegard√© (si disponible)"""
    global model, vectorizer
    
    try:
        with open('spam_model.pkl', 'rb') as f:
            model = pickle.load(f)
        
        with open('vectorizer.pkl', 'rb') as f:
            vectorizer = pickle.load(f)
        
        print("‚úÖ Mod√®le charg√© depuis les fichiers")
        return True
    except FileNotFoundError:
        print("‚ö†Ô∏è Aucun mod√®le sauvegard√© trouv√©")
        return False

# ============================================
# INITIALISATION AU D√âMARRAGE
# ============================================

@app.on_event("startup")
async def startup_event():
    """
    Fonction ex√©cut√©e au d√©marrage de l'application
    - Tente de charger un mod√®le existant
    - Sinon, entra√Æne un nouveau mod√®le
    """
    print("\n" + "="*50)
    print("üöÄ D√âMARRAGE DU D√âTECTEUR DE SPAM")
    print("="*50 + "\n")
    
    # Essai de chargement d'un mod√®le existant
    if not load_model():
        # Si pas de mod√®le, on en entra√Æne un nouveau
        print("üîÑ Entra√Ænement d'un nouveau mod√®le...")
        
        # 1. Chargement des donn√©es
        df = load_dataset()
        
        # 2. Pr√©traitement
        df = preprocess_data(df)
        
        # 3. S√©paration train/test (80% / 20%)
        X = df['message']
        y = df['label_num']
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"üìä Donn√©es divis√©es :")
        print(f"   - Entra√Ænement : {len(X_train)} messages")
        print(f"   - Test : {len(X_test)} messages\n")
        
        # 4. Vectorisation
        X_train_vec, X_test_vec = create_features(X_train, X_test)
        
        # 5. Entra√Ænement
        train_model(X_train_vec, y_train, X_test_vec, y_test)
        
        # 6. Sauvegarde
        save_model()
    
    print("\n" + "="*50)
    print("‚úÖ D√âTECTEUR DE SPAM PR√äT !")
    print("="*50 + "\n")

# ============================================
# ROUTES DE L'API
# ============================================

@app.get("/")
async def root():
    """Page d'accueil de l'API"""
    return {
        "message": "D√©tecteur de Spam SMS - API",
        "version": "1.0.0",
        "endpoints": {
            "predict": "/predict (POST)",
            "stats": "/stats (GET)",
            "health": "/health (GET)"
        }
    }

@app.get("/health")
async def health_check():
    """V√©rifie que l'API fonctionne"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "vectorizer_loaded": vectorizer is not None
    }

@app.get("/stats")
async def get_stats():
    """Retourne les statistiques du mod√®le"""
    if not model_stats:
        raise HTTPException(status_code=503, detail="Mod√®le non entra√Æn√©")
    
    return {
        "accuracy": f"{model_stats['accuracy']*100:.2f}%",
        "confusion_matrix": model_stats['confusion_matrix'],
        "details": "Matrice de confusion : [[True Ham, False Spam], [False Ham, True Spam]]"
    }

@app.post("/predict", response_model=PredictionResponse)
async def predict_spam(message: Message):
    """
    Endpoint principal : pr√©dit si un message est spam ou ham
    
    Param√®tres :
    - message.text : Le texte du message √† analyser
    
    Retour :
    - prediction : "spam" ou "ham"
    - confidence : Score de confiance en %
    """
    if model is None or vectorizer is None:
        raise HTTPException(
            status_code=503, 
            detail="Mod√®le non disponible. Veuillez r√©essayer dans quelques instants."
        )
    
    try:
        # 1. Vectorisation du message
        message_vec = vectorizer.transform([message.text])
        
        # 2. Pr√©diction
        prediction = model.predict(message_vec)[0]
        
        # 3. Calcul de la confiance (probabilit√©)
        proba = model.predict_proba(message_vec)[0]
        confidence = max(proba) * 100  # Confiance en %
        
        # 4. Conversion du r√©sultat
        label = "spam" if prediction == 1 else "ham"
        
        return PredictionResponse(
            text=message.text,
            prediction=label,
            confidence=round(confidence, 2)
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur de pr√©diction : {str(e)}")

# ============================================
# LANCEMENT DE L'APPLICATION
# ============================================

if __name__ == "__main__":
    import uvicorn
    
    # Lancement du serveur
    uvicorn.run(
        "app:app",
        host="0.0.0.0",  # √âcoute sur toutes les interfaces
        port=8000,
        reload=True      # Rechargement automatique en d√©veloppement
    )