# ============================================
# APP.PY - D√âTECTEUR DE SPAM HYBRIDE INTELLIGENT
# ============================================
# Architecture : Logistic Regression + Analyse de patterns
# Niveau 1 : Filtre rapide ML (0.1 sec)
# Niveau 2 : Analyse approfondie des patterns dangereux
# ============================================

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import pickle
import requests
import io
import zipfile

# ============================================
# CONFIGURATION DE L'APPLICATION
# ============================================

app = FastAPI(
    title="D√©tecteur de Spam Hybride",
    description="IA intelligente combinant ML et analyse de patterns",
    version="2.0.0"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# MOD√àLES DE DONN√âES
# ============================================

class Message(BaseModel):
    text: str

class PredictionResponse(BaseModel):
    text: str
    prediction: str
    confidence: float
    method: str  # "ml_fast" ou "ml_deep_analysis"
    danger_signals: list  # Liste des signaux de danger d√©tect√©s

# ============================================
# VARIABLES GLOBALES
# ============================================

model = None
vectorizer = None
model_stats = {}

# ============================================
# ANALYSEUR DE PATTERNS DANGEREUX
# ============================================
# Cette classe d√©tecte des signaux de spam au-del√† des mots

class DangerPatternAnalyzer:
    """
    Analyse approfondie des patterns de spam
    D√©tecte : URLs suspectes, urgence, MAJUSCULES, argent, num√©ros
    """
    
    def __init__(self):
        # Mots-cl√©s d'urgence (psychologie de la pression)
        self.urgency_words = [
            'urgent', 'now', 'immediately', 'hurry', 'limited time',
            'expire', 'last chance', 'act now', 'don\'t wait',
            'urgent', 'maintenant', 'imm√©diatement', 'vite', 'limit√©'
        ]
        
        # Mots-cl√©s d'argent (app√¢t financier)
        self.money_words = [
            'free', 'win', 'winner', 'cash', 'prize', 'million',
            'dollars', 'euros', '$', '‚Ç¨', 'money', 'rich', 'earn',
            'gratuit', 'gagner', 'gagnant', 'prix', 'argent'
        ]
        
        # Mots de demande d'action (phishing)
        self.action_words = [
            'click', 'call', 'verify', 'confirm', 'update', 'download',
            'install', 'register', 'claim', 'redeem',
            'cliquer', 'appeler', 'v√©rifier', 'confirmer', 't√©l√©charger'
        ]
    
    def analyze(self, text):
        """
        Analyse compl√®te d'un message
        Retourne : score de danger (0-100) et liste des signaux
        """
        text_lower = text.lower()
        danger_score = 0
        signals = []
        
        # 1. D√âTECTION D'URLs (les spams contiennent souvent des liens)
        urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
        shortened_urls = re.findall(r'\b(?:bit\.ly|tinyurl|goo\.gl|ow\.ly|t\.co)/\w+', text_lower)
        
        if urls or shortened_urls:
            danger_score += 20
            signals.append(f"üîó Contient {len(urls) + len(shortened_urls)} URL(s)")
        
        # 2. D√âTECTION DE NUM√âROS DE T√âL√âPHONE
        # Formats : +33, 06, 07, (555) 123-4567, etc.
        phone_patterns = [
            r'\+?\d{1,3}[-.\s]?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}',
            r'\b0[6-7]\d{8}\b',  # Fran√ßais
            r'\b\d{3}[-.\s]?\d{3}[-.\s]?\d{4}\b'  # US
        ]
        phones = []
        for pattern in phone_patterns:
            phones.extend(re.findall(pattern, text))
        
        if phones:
            danger_score += 15
            signals.append(f"üìû Contient {len(phones)} num√©ro(s) de t√©l√©phone")
        
        # 3. D√âTECTION DE MAJUSCULES EXCESSIVES
        # Les spammeurs crient pour attirer l'attention
        if text.isupper() or len([c for c in text if c.isupper()]) / max(len(text), 1) > 0.5:
            danger_score += 25
            signals.append("üîä TEXTE EN MAJUSCULES (crie pour attirer l'attention)")
        
        # 4. D√âTECTION DE POINTS D'EXCLAMATION MULTIPLES
        exclamations = text.count('!')
        if exclamations >= 3:
            danger_score += 15
            signals.append(f"‚ùó {exclamations} points d'exclamation (urgence artificielle)")
        
        # 5. D√âTECTION DE MOTS D'URGENCE
        urgency_count = sum(1 for word in self.urgency_words if word in text_lower)
        if urgency_count > 0:
            danger_score += urgency_count * 10
            signals.append(f"‚è∞ {urgency_count} mot(s) d'urgence d√©tect√©(s)")
        
        # 6. D√âTECTION DE MOTS D'ARGENT
        money_count = sum(1 for word in self.money_words if word in text_lower)
        if money_count > 0:
            danger_score += money_count * 8
            signals.append(f"üí∞ {money_count} mot(s) li√©(s) √† l'argent")
        
        # 7. D√âTECTION DE DEMANDES D'ACTION
        action_count = sum(1 for word in self.action_words if word in text_lower)
        if action_count > 0:
            danger_score += action_count * 7
            signals.append(f"üëÜ {action_count} demande(s) d'action")
        
        # 8. D√âTECTION D'EMAILS SUSPECTS
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text)
        if emails:
            danger_score += 10
            signals.append(f"üìß Contient {len(emails)} adresse(s) email")
        
        # 9. D√âTECTION DE SYMBOLES MON√âTAIRES
        currency_symbols = len(re.findall(r'[$‚Ç¨¬£¬•‚Çπ]', text))
        if currency_symbols >= 2:
            danger_score += 12
            signals.append(f"üíµ {currency_symbols} symboles mon√©taires")
        
        # 10. D√âTECTION DE MOTS R√âP√âT√âS (technique de spam)
        words = text_lower.split()
        word_counts = {}
        for word in words:
            if len(word) > 3:  # Ignore les petits mots
                word_counts[word] = word_counts.get(word, 0) + 1
        
        repeated = [word for word, count in word_counts.items() if count >= 3]
        if repeated:
            danger_score += len(repeated) * 5
            signals.append(f"üîÅ Mots r√©p√©t√©s : {', '.join(repeated[:3])}")
        
        # Limiter le score √† 100
        danger_score = min(danger_score, 100)
        
        return danger_score, signals

# Initialisation de l'analyseur
pattern_analyzer = DangerPatternAnalyzer()

# ============================================
# CHARGEMENT DU DATASET
# ============================================

def load_dataset():
    """Charge le dataset SMS Spam Collection"""
    print("üì• T√©l√©chargement du dataset...")
    
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:
            with zip_file.open('SMSSpamCollection') as f:
                df = pd.read_csv(f, sep='\t', names=['label', 'message'], encoding='latin-1')
        
        print(f"‚úÖ Dataset charg√© : {len(df)} messages")
        print(f"   - Ham : {len(df[df['label']=='ham'])}")
        print(f"   - Spam : {len(df[df['label']=='spam'])}")
        
        return df
    
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        # Dataset minimal de secours
        return pd.DataFrame({
            'label': ['ham', 'spam', 'ham', 'spam', 'ham', 'spam'],
            'message': [
                'Hello, how are you doing today?',
                'WINNER! FREE cash prize! Click NOW!!!',
                'Meeting at 3pm tomorrow, see you there',
                'Congratulations! Call +1-555-0100 to claim your prize',
                'Thanks for your help with the project',
                'URGENT: Verify your account NOW at http://fake-bank.com'
            ]
        })

# ============================================
# PR√âTRAITEMENT AVANC√â
# ============================================

def preprocess_data(df):
    """Pr√©pare les donn√©es pour l'entra√Ænement"""
    print("üßπ Pr√©traitement des donn√©es...")
    
    # Conversion des labels
    df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})
    
    # Suppression des lignes vides
    df = df.dropna()
    
    # Suppression des doublons (am√©liore la qualit√©)
    df = df.drop_duplicates(subset=['message'])
    
    print(f"‚úÖ {len(df)} messages uniques pr√™ts")
    return df

# ============================================
# VECTORISATION TF-IDF AM√âLIOR√âE
# ============================================

def create_features(X_train, X_test):
    """
    Vectorisation TF-IDF optimis√©e
    - N-grams (1,3) : mots seuls + paires + triplets
    - Plus de features : 5000 au lieu de 3000
    - Min_df : ignore les mots trop rares
    """
    print("üî¢ Vectorisation TF-IDF am√©lior√©e...")
    
    global vectorizer
    vectorizer = TfidfVectorizer(
        lowercase=True,
        stop_words='english',
        max_features=5000,        # Plus de mots = plus pr√©cis
        ngram_range=(1, 3),       # Unigrams, bigrams, trigrams
        min_df=2,                 # Ignore les mots pr√©sents dans < 2 docs
        max_df=0.8,               # Ignore les mots trop fr√©quents
        strip_accents='unicode',  # G√®re les accents
        token_pattern=r'\b\w+\b'  # Garde les mots complets
    )
    
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    
    print(f"‚úÖ {X_train_vec.shape[1]} features cr√©√©es")
    return X_train_vec, X_test_vec

# ============================================
# ENTRA√éNEMENT - LOGISTIC REGRESSION
# ============================================
# Pourquoi Logistic Regression au lieu de Na√Øve Bayes ?
# - Plus pr√©cis (92-95% vs 85-90%)
# - G√®re mieux les relations entre mots
# - Toujours tr√®s rapide
# - Donne de vraies probabilit√©s calibr√©es

def train_model(X_train, y_train, X_test, y_test):
    """Entra√Æne le mod√®le Logistic Regression"""
    print("ü§ñ Entra√Ænement Logistic Regression...")
    
    global model, model_stats
    
    # Cr√©ation du mod√®le
    model = LogisticRegression(
        C=1.0,              # R√©gularisation (1.0 = √©quilibr√©)
        max_iter=1000,      # It√©rations max
        solver='lbfgs',     # Algorithme d'optimisation
        random_state=42     # Reproductibilit√©
    )
    
    # Entra√Ænement
    model.fit(X_train, y_train)
    
    # Pr√©dictions
    y_pred = model.predict(X_test)
    
    # M√©triques
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred, target_names=['Ham', 'Spam'])
    
    model_stats = {
        'accuracy': float(accuracy),
        'confusion_matrix': conf_matrix.tolist(),
        'classification_report': class_report,
        'model_type': 'Logistic Regression (Hybride)'
    }
    
    print(f"‚úÖ Mod√®le entra√Æn√© !")
    print(f"   Pr√©cision : {accuracy*100:.2f}%")
    print(f"\nüìä Rapport :\n{class_report}")
    
    return model

# ============================================
# SYST√àME DE PR√âDICTION HYBRIDE
# ============================================

def predict_hybrid(text):
    """
    Pr√©diction hybride intelligente
    
    Niveau 1 : ML rapide (Logistic Regression)
    Niveau 2 : Analyse de patterns si confiance < 90%
    
    Retourne : prediction, confidence, method, signals
    """
    
    # NIVEAU 1 : Pr√©diction ML
    text_vec = vectorizer.transform([text])
    prediction_num = model.predict(text_vec)[0]
    proba = model.predict_proba(text_vec)[0]
    ml_confidence = max(proba) * 100
    
    # NIVEAU 2 : Analyse de patterns
    danger_score, signals = pattern_analyzer.analyze(text)
    
    # D√âCISION HYBRIDE
    # Si ML est tr√®s confiant (>90%), on lui fait confiance
    if ml_confidence >= 90:
        final_prediction = "spam" if prediction_num == 1 else "ham"
        final_confidence = ml_confidence
        method = "ml_fast"
    
    # Sinon, on combine ML + patterns
    else:
        # Pond√©ration : 70% ML + 30% patterns
        ml_spam_score = proba[1] * 100  # Probabilit√© de spam selon ML
        combined_score = (ml_spam_score * 0.7) + (danger_score * 0.3)
        
        final_prediction = "spam" if combined_score >= 50 else "ham"
        final_confidence = combined_score if combined_score >= 50 else (100 - combined_score)
        method = "ml_deep_analysis"
        
        # Ajout d'un signal pour expliquer la d√©cision
        if method == "ml_deep_analysis":
            signals.insert(0, f"üß† Analyse approfondie (ML: {ml_spam_score:.1f}% + Patterns: {danger_score}%)")
    
    return final_prediction, final_confidence, method, signals

# ============================================
# SAUVEGARDE / CHARGEMENT
# ============================================

def save_model():
    """Sauvegarde le mod√®le"""
    print("üíæ Sauvegarde du mod√®le...")
    with open('spam_model_hybrid.pkl', 'wb') as f:
        pickle.dump(model, f)
    with open('vectorizer_hybrid.pkl', 'wb') as f:
        pickle.dump(vectorizer, f)
    print("‚úÖ Mod√®le sauvegard√©")

def load_model():
    """Charge le mod√®le sauvegard√©"""
    global model, vectorizer
    try:
        with open('spam_model_hybrid.pkl', 'rb') as f:
            model = pickle.load(f)
        with open('vectorizer_hybrid.pkl', 'rb') as f:
            vectorizer = pickle.load(f)
        print("‚úÖ Mod√®le charg√©")
        return True
    except FileNotFoundError:
        print("‚ö†Ô∏è Aucun mod√®le sauvegard√©")
        return False

# ============================================
# INITIALISATION AU D√âMARRAGE
# ============================================

@app.on_event("startup")
async def startup_event():
    """Entra√Æne ou charge le mod√®le au d√©marrage"""
    print("\n" + "="*60)
    print("üöÄ D√âTECTEUR DE SPAM HYBRIDE v2.0")
    print("="*60 + "\n")
    
    if not load_model():
        df = load_dataset()
        df = preprocess_data(df)
        
        X = df['message']
        y = df['label_num']
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"üìä Train: {len(X_train)} | Test: {len(X_test)}\n")
        
        X_train_vec, X_test_vec = create_features(X_train, X_test)
        train_model(X_train_vec, y_train, X_test_vec, y_test)
        save_model()
    
    print("\n" + "="*60)
    print("‚úÖ SYST√àME PR√äT - Hybride ML + Analyse de Patterns")
    print("="*60 + "\n")

# ============================================
# ROUTES DE L'API
# ============================================

@app.get("/")
async def root():
    """Page d'accueil"""
    return {
        "name": "D√©tecteur de Spam Hybride",
        "version": "2.0.0",
        "description": "IA combinant ML et analyse de patterns",
        "endpoints": {
            "predict": "/predict (POST)",
            "stats": "/stats (GET)",
            "health": "/health (GET)"
        }
    }

@app.get("/health")
async def health_check():
    """V√©rifie que l'API fonctionne"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "model_type": "Logistic Regression + Pattern Analysis"
    }

@app.get("/stats")
async def get_stats():
    """Retourne les statistiques"""
    if not model_stats:
        raise HTTPException(status_code=503, detail="Mod√®le non entra√Æn√©")
    
    return {
        "accuracy": f"{model_stats['accuracy']*100:.2f}%",
        "model_type": model_stats.get('model_type', 'Unknown'),
        "confusion_matrix": model_stats['confusion_matrix']
    }

@app.post("/predict", response_model=PredictionResponse)
async def predict_spam(message: Message):
    """
    Endpoint principal : pr√©diction hybride
    
    Combine :
    - Machine Learning (Logistic Regression)
    - Analyse de patterns dangereux
    """
    if model is None or vectorizer is None:
        raise HTTPException(status_code=503, detail="Mod√®le non disponible")
    
    try:
        # Pr√©diction hybride
        prediction, confidence, method, signals = predict_hybrid(message.text)
        
        return PredictionResponse(
            text=message.text,
            prediction=prediction,
            confidence=round(confidence, 2),
            method=method,
            danger_signals=signals
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur : {str(e)}")

# ============================================
# LANCEMENT
# ============================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True)